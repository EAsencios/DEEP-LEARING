{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vision_transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqBHgaTtUgDadDvPxr/Uuj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EAsencios/DEEP-LEARING/blob/master/vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "jWBXK34oZkDx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zF4t3Q3JG-ht"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparate the data"
      ],
      "metadata": {
        "id": "uaDDWIBsZsQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print('X_train shape: {} - y_train shape: {}'.format(X_train.shape, y_train.shape))\n",
        "print(\"X_test shape: {} - y_test shape: {}\".format(X_test.shape, y_train.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlYgqsuuH2Y9",
        "outputId": "d9d22370-2aa9-43e6-a7eb-e2f9dc151595"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "X_test shape: (10000, 32, 32, 3) - y_test shape: (50000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the hyperparameters"
      ],
      "metadata": {
        "id": "wlEMAbGSZy__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72   # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extraxt from the input images\n",
        "num_patches = (image_size // patch_size)**2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "                     projection_dim*2,\n",
        "                     projection_dim,\n",
        "]   # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024] # Size of the dense layers of the final classifier"
      ],
      "metadata": {
        "id": "wdMm7eDrLFvb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use data augmentation"
      ],
      "metadata": {
        "id": "EZXZ1U-JaAeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import hexversion\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "     layers.Normalization(),\n",
        "     layers.Resizing(image_size, image_size),\n",
        "     layers.RandomFlip(\"horizontal\"),\n",
        "     layers.RandomRotation(factor=0.02),\n",
        "     layers.RandomZoom(height_factor=.2, width_factor=.2),\n",
        "     ],\n",
        "     name = \"data_augmentation\",\n",
        ")\n",
        "\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(X_train)"
      ],
      "metadata": {
        "id": "2XSEh4ZqQOsh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement multilayer perceptron"
      ],
      "metadata": {
        "id": "Zzh92SQLaJb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "  for units in hidden_units:\n",
        "    x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "nAD_WLxWYcFH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement pach creation as a layer"
      ],
      "metadata": {
        "id": "8Lt8Hb-MaVCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "  def __init__(self, patch_size):\n",
        "      super(Patches, self).__init__()\n",
        "      self.patch_size = patch_size\n",
        "\n",
        "  def call(self, images):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    batches = tf.image.extract_patches(images=images,\n",
        "                                       sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "                                       strides=[1, self.patch_size, self.patch_size, 1],\n",
        "                                       rates=[1, 1, 1, 1],\n",
        "                                       padding=\"VALID\",\n",
        "                                       )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "    return patches"
      ],
      "metadata": {
        "id": "6Uv7rPRcZfes"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's display patches for a sample image"
      ],
      "metadata": {
        "id": "O-dYxfrUh_Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import resize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "image = X_train[np.random.choice(range(X_train.shape[0]))]\n",
        "plt.imshow(image.astype('uint8'))\n",
        "plt.axis('off')\n",
        "\n",
        "resized_image = tf.image.resize(tf.convert_to_tensor([image]), size=(image_size, image_size))\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "EKDnBr8qiMs6",
        "outputId": "13bf4ad7-ef94-428c-c243-fa56addaffb6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-51dfab58198f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Image size: {image_size} X {image_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-9d5d0f2933ff>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                        \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VALID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                        )\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpatch_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_dims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: Exception encountered when calling layer \"patches_1\" (type Patches).\n\nlocal variable 'patches' referenced before assignment\n\nCall arguments received:\n  • images=tf.Tensor(shape=(1, 72, 72, 3), dtype=float32)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}